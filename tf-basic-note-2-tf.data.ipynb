{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.data and optimizer\n",
    "\n",
    "TF 的基础笔记 2，灵感来源于 [Stanford CS20si 课程](http://web.stanford.edu/class/cs20si/syllabus.html)，enjoy！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.data\n",
    "- 使用 placeholder 和 feed_dict 的好处是数据的控制是在 tf 之外的，很 pythonic，并且容易 shuffle, batch, random。但是不好的地方时，数据处理是单线程的，降低速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, n_samples = utils.read_birth_life_data(DATA_FILE)\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "with tf.Session() as sess:\n",
    "       ...\n",
    "    # Step 8: train the model\n",
    "    for i in range(100): # run 100 epochs\n",
    "        for x, y in data:\n",
    "            # Session runs train_op to minimize loss\n",
    "            sess.run(optimizer, feed_dict={X: x, Y:y}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将数据存在 tf.data.Dataset 对象中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 初始化\n",
    "# features 和 labels 都是 tensor\n",
    "tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "# 因为 tf 和 np 是无缝集成的，因此也可以传入 np array，所以初始化可以变为：\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data[:,0], data[:,1]))\n",
    "# sanity check\n",
    "print(dataset.output_types) # >> (tf.float32, tf.float32)\n",
    "print(dataset.output_shapes) # >> (TensorShape([]), TensorShape([]))\n",
    "# 也可以直接从文件中创建 Dataset\n",
    "tf.data.TextLineDataset(filenames)\n",
    "tf.data.FixedLengthRecordDataset(filenames)\n",
    "tf.data.TFRecordDataset(filenames)\n",
    "# 2. 创建迭代器\n",
    "# 创建迭代一次的迭代器\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "X, Y = iterator.get_next()\n",
    "# 使用方法\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([X, Y])) # >> [1.822, 74.82825]\n",
    "    print(sess.run([X, Y])) # >> [3.869, 70.81949]\n",
    "    print(sess.run([X, Y])) # >> [3.911, 72.15066]\n",
    "# 可以直接调用 optimizer 来执行运算，再也不需要 feed_dict 了\n",
    "# 注意 TF 并没有捕捉 OutOfRangeError 异常，因此我们需要自己捕捉\n",
    "for i in range(100): # train the model 100 epochs\n",
    "        total_loss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                sess.run([optimizer]) \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "# 创建可以反复迭代的迭代器\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "...\n",
    "for i in range(100): \n",
    "        sess.run(iterator.initializer) \n",
    "        total_loss = 0\n",
    "        try:\n",
    "            while True:\n",
    "                sess.run([optimizer]) \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "# 3. 设置 batch，shuffle，repeat 甚至是数据转换\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.repeat(100)\n",
    "dataset = dataset.batch(128)\n",
    "dataset = dataset.map(lambda x: tf.one_hot(x, 10)) # convert each element of dataset to one_hot vector\n",
    "\n",
    "# 数据比较\n",
    "# 在 Macbook Pro with 2.7 GHz Intel Core i5\n",
    "# with placeholder took on average 9.05271519 seconds\n",
    "# with tf.data took on average 6.12285947 seconds\n",
    "# 32.4% faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "sess.run([optimizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer 是一个 operator，它会在图中寻找 loss 依赖的节点，并且对可以训练的节点做自动偏微分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable 中的 trainerable 参数控制是否需要被更新。\n",
    "# 一个例子是 global_step 变量，就需要将它的 trainerable 设置为 false\n",
    "global_step = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "learning_rate = 0.01 * 0.99 ** tf.cast(global_step, tf.float32)\n",
    "increment_step = global_step.assign_add(1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate) # learning rate can be a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以设置让 optimizer 只计算指定的梯度。在计算的过程中，你还可以更改算出的梯度，并反馈回去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# compute the gradients for a list of variables.\n",
    "grads_and_vars = optimizer.compute_gradients(loss, <list of variables>)\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example, subtract each of them by 1.\n",
    "subtracted_grads_and_vars = [(gv[0] - 1.0, gv[1]) for gv in grads_and_vars]\n",
    "# ask the optimizer to apply the subtracted gradients.\n",
    "optimizer.apply_gradients(subtracted_grads_and_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer 会自动计算图中所有变量的导数。但是你也可以让 tf 去计算某些特别变量的导数，使用 tf.gradients。\n",
    "- 这个方法会计算 ys 对 xs 的偏导数，是一个 tensor 或者 tensor 的列表\n",
    "- grad_ys 是用来记录 ys 的梯度的，和 ys 形状相同\n",
    "\n",
    "当只训练模型的一部分时，这个方法很有用，例如使用 tf.gradients() 来计算 loss 相对于中间层网络的梯度 G。然后我们只需要优化中间层的输出 M 和 M+G 的差别最小即可。这只需要更新神经网络的前半部分。（差别最小，即收敛。最优化的目标就是梯度最小化。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gradients(\n",
    "    ys,\n",
    "    xs,\n",
    "    grad_ys=None,\n",
    "    name='gradients',\n",
    "    colocate_gradients_with_ops=False,\n",
    "    gate_gradients=False,\n",
    "    aggregation_method=None,\n",
    "    stop_gradients=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面罗列出一系列的优化函数。目前来看，Adam 优化是最好的选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Optimizer\n",
    "tf.train.GradientDescentOptimizer\n",
    "tf.train.AdadeltaOptimizer\n",
    "tf.train.AdagradOptimizer\n",
    "tf.train.AdagradDAOptimizer\n",
    "tf.train.MomentumOptimizer\n",
    "tf.train.AdamOptimizer\n",
    "tf.train.FtrlOptimizer\n",
    "tf.train.ProximalGradientDescentOptimizer\n",
    "tf.train.ProximalAdagradOptimizer\n",
    "tf.train.RMSPropOptimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
