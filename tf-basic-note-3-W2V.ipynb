{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "* 编写 data 读取\n",
    "* 编写 w2v 的图构建代码\n",
    "* 训练\n",
    "\n",
    "通过 w2v 代码学习 ... 的使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf name scope, 将节点在 tensor board 中归并到一起\n",
    "with tf.name_scope('data'):\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "# tf variable scope\n",
    "# 在相同的 scope 中，variable 可以被重新使用\n",
    "# 例如下面代码，变量不会出现重复定义的错误\n",
    "# ValueError: Variable h1_weights already exists, disallowed. Did you mean to set reuse=True in VarScope?\n",
    "with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
    "        w = tf.get_variable(\"weights\", [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n",
    "        b = tf.get_variable(\"biases\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(x, w) + b\n",
    "# 或者使用下面的方法\n",
    "with tf.variable_scope('two_layers') as scope:\n",
    "    logits1 = two_hidden_layers(x1)\n",
    "    scope.reuse_variables()\n",
    "    logits2 = two_hidden_layers(x2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.train.Saver 保存训练中的变量，不保存 Graph\n",
    "saver = tf.train.Saver()\n",
    "# step 记录特定的训练步数\n",
    "saver.save(sess, 'ckpt_path/model_name', global_step=step)\n",
    "\n",
    "# 选择保存特定的变量\n",
    "v1 = tf.Variable(..., name='v1') \n",
    "v2 = tf.Variable(..., name='v2') \n",
    "saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
    "saver = tf.train.Saver([v1, v2])\n",
    "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]}) # similar to a dict\n",
    "\n",
    "# 从 ckpt 中恢复图，同样需要先构建好图\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "# check if there is a checkpoint and valid checkpoint path\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "     saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary -> 可以通过 tensor board 查看训练的变化图\n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", self.loss)\n",
    "    tf.summary.scalar(\"accuracy\", self.accuracy)            \n",
    "    tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "loss_batch, _, summary = sess.run([loss, optimizer, summary_op])\n",
    "writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "writer.add_summary(summary, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加随机种子\n",
    "# op level\n",
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "d = tf.random_uniform([], -10, 10, seed=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "    print(sess.run(d)) # >> 3.57493\n",
    "\n",
    "# session level\n",
    "c = tf.random_uniform([], -10, 10, seed=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "    print(sess.run(c)) # >> -5.97319\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> 3.57493\n",
    "\n",
    "# graph level\n",
    "tf.set_random_seed(2)\n",
    "c = tf.random_uniform([], -10, 10)\n",
    "d = tf.random_uniform([], -10, 10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c)) # >> -4.00752\n",
    "    print(sess.run(d)) # >> -2.98339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局训练步数记录\n",
    "# 注：trainable 设置为 False\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "# 传给 optimizer，这样 optimizer 就可以做 rate decay，并且自动更新 optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto differentiation\n",
    "# 根据图的定义，tf 会自动根据反向依赖关系，求出导数\n",
    "# 使用链式法则，计算图中的导数求解很容易\n",
    "tf.gradients(ys, xs, grad_ys=None, ...) # 求 ys 对 xs 的导数\n",
    "tf.stop_gradient(input, name=None) # 防止输入也被自动求导\n",
    "tf.clip_by_value(t, clip_value_min, clip_value_max, name=None) # 梯度截断\n",
    "tf.clip_by_norm(t, clip_norm, axes=None, name=None) # 梯度截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    file_path = 'data/text8.zip'\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        words = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(words, vocab_size, visual_fld):\n",
    "    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words and write it to\n",
    "    visualization/vocab.tsv\n",
    "    \"\"\"\n",
    "    file = open(os.path.join(visual_fld, 'vocab.tsv'), 'w')\n",
    "    \n",
    "    dictionary = dict()\n",
    "    count = [('UNK', -1)]\n",
    "    index = 0\n",
    "    count.extend(Counter(words).most_common(vocab_size - 1))\n",
    "    \n",
    "    for word, _ in count:\n",
    "        dictionary[word] = index\n",
    "        index += 1\n",
    "        file.write(word + '\\n')\n",
    "    \n",
    "    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    file.close()\n",
    "    return dictionary, index_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, visual_fld = 50000, 'visualization'\n",
    "# dictionary, index_dict = build_vocab(words, vocab_size, visual_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words]\n",
    "\n",
    "# index_words = convert_words_to_index(words, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_window = 5\n",
    "def generate_sample(index_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # get a random target before the center word\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # get a random target after the center wrod\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target\n",
    "single_gen = generate_sample(index_words, skip_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def batch_gen():\n",
    "    local_dest = 'data/text8.zip'\n",
    "    words = read_data()\n",
    "    dictionary, _ = build_vocab(words, vocab_size, visual_fld)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words           # to save memory\n",
    "    single_gen = generate_sample(index_words, skip_window)\n",
    "    \n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dataset = tf.data.Dataset.from_generator(batch_gen, \n",
    "                            (tf.int32, tf.int32), \n",
    "                            (tf.TensorShape([batch_size]), tf.TensorShape([batch_size, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "num_sampled = 64\n",
    "learning_rate = 1.0\n",
    "# 构建图\n",
    "# 读取训练数据\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    center_words, target_words = iterator.get_next()\n",
    "\n",
    "# 定义 embedding\n",
    "# embedding lookup\n",
    "with tf.name_scope('embed'):\n",
    "    embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                    shape=[vocab_size, embed_size],\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "    embed = tf.nn.embedding_lookup(embed_matrix, center_words, name='embedding')\n",
    "\n",
    "# 定义损失\n",
    "with tf.name_scope('loss'):\n",
    "    nce_weight = tf.get_variable('nce_weight', shape=[vocab_size, embed_size],\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=1.0/(embed_size ** 0.5)))\n",
    "    nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([vocab_size]))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                        biases=nce_bias,\n",
    "                                        labels=target_words,\n",
    "                                        inputs=embed,\n",
    "                                        num_sampled=64,\n",
    "                                        num_classes=vocab_size), name='loss')\n",
    "\n",
    "# 定义优化函数\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "writer = tf.summary.FileWriter('graphs/word2vec_simple', tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4: 271.0\n",
      "Average loss at step 9: 247.7\n",
      "Average loss at step 14: 237.9\n",
      "Average loss at step 19: 229.6\n",
      "Average loss at step 24: 232.7\n",
      "Average loss at step 29: 230.0\n",
      "Average loss at step 34: 224.5\n",
      "Average loss at step 39: 207.1\n",
      "Average loss at step 44: 208.5\n",
      "Average loss at step 49: 220.3\n",
      "Average loss at step 54: 216.3\n",
      "Average loss at step 59: 192.6\n",
      "Average loss at step 64: 200.2\n",
      "Average loss at step 69: 214.4\n",
      "Average loss at step 74: 210.7\n",
      "Average loss at step 79: 190.5\n",
      "Average loss at step 84: 165.5\n",
      "Average loss at step 89: 188.5\n",
      "Average loss at step 94: 200.5\n",
      "Average loss at step 99: 198.8\n"
     ]
    }
   ],
   "source": [
    "# 运行\n",
    "sess = tf.Session()\n",
    "sess.run(iterator.initializer)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_loss = 0.0\n",
    "num_train_steps = 100\n",
    "skip_step = 5\n",
    "for index in range(num_train_steps):\n",
    "    try:\n",
    "        loss_batch, _ = sess.run([loss, optimizer])\n",
    "        total_loss += loss_batch\n",
    "        if (index+1) % 5 == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index, total_loss / skip_step))\n",
    "            total_loss = 0\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        sess.run(iterator.initializer)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 projector 对词向量进行可视化\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "final_embed_matrix = sess.run(embed_matrix)\n",
    "embedding_var = tf.Variable(final_embed_matrix[:1000], name='embedding')\n",
    "sess.run(embedding_var.initializer)\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "summary_writer = tf.summary.FileWriter('visualization')\n",
    "projector.visualize_embeddings(summary_writer, config)\n",
    "saver_embed = tf.train.Saver([embedding_var])\n",
    "saver_embed.save(sess, os.path.join('visualization', 'model.ckpt'), 1)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
