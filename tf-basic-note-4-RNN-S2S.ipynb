{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN, LSTM and S2S\n",
    "\n",
    "#### 前馈神经网络 vs RNN\n",
    "1 DAG vs 有环的计算图\n",
    "2 信号往同一个方向传 vs 信号被传回到同一个神经元\n",
    "3 每一层有独立的变量 vs 每一步共用变量\n",
    "\n",
    "#### BPTT\n",
    "1 每一步的梯度求和后更新变量\n",
    "2 步数很多时将需要很多的计算\n",
    "\n",
    "#### Truncated BPTT\n",
    "使用有限的步数，提高训练速度\n",
    "\n",
    "因为梯度消失/梯度爆炸，RNN 并不能保存长期的记录\n",
    "\n",
    "#### LSTM\n",
    "- input gate: 这次的输入需要放进来多少\n",
    "- forget gate：上次的状态需要考虑多少\n",
    "- output gate：隐藏的状态需要放出多少\n",
    "- candidate gate\n",
    "- memory cell\n",
    "\n",
    "#### RNN 能做什么\n",
    "- 计算句子的相似度\n",
    "- 机器翻译\n",
    "- 文本生成\n",
    "- 文本归纳\n",
    "- 看图说话 - 将 CNN 的特征作为输入\n",
    "\n",
    "#### 语言模型\n",
    "- word level\n",
    "- char level\n",
    "\n",
    "#### 语言模型缺点\n",
    "- n-gram\n",
    "  - 基于先前的 n-gram 来推测下一个词是什么\n",
    "  - 大量的词典\n",
    "  - 没办法生成词典外的词\n",
    "  - 需要大量内存\n",
    "- char level\n",
    "  - 非常小的词典\n",
    "  - 不需要 word embedding\n",
    "  - 训练快速\n",
    "  - 但效率低，容易出现瞎说\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的 tf.nn.rnn_cell\n",
    "# BasicRNNCell 基本的 RNN 单元\n",
    "# RNNCell: Abstract object representing an RNN cell\n",
    "# BasicLSTMCell: 基本的 LSTM 单元\n",
    "# LSTMCell: LSTM 单元\n",
    "# GRUCell: GRU 单元\n",
    "\n",
    "# 单个单元\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "# 将单元组合\n",
    "layers = [tf.nn.rnn_cell.GRUCell(size) for size in hidden_sizes] # 每个隐藏层的状态 size\n",
    "cells = tf.nn.rnn_cell.MultiRNNCell(layers) # 多层的 RNN\n",
    "# dynamic_rnn，使用 tf.While 来动态构建图。并且能喂给变长的 batch\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)\n",
    "# 对 seq 做 padding，loss 只计算非 padding 的\n",
    "full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)\n",
    "loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))\n",
    "# 告诉模型真实的 seq 长度，让模型只预测真实的 token\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tricks\n",
    "# 使用不同的激活函数\n",
    "tf.nn.relu\n",
    "tf.nn.relu6\n",
    "tf.nn.crelu\n",
    "tf.nn.elu\n",
    "# 更多的激活函数\n",
    "tf.nn.softplus\n",
    "tf.nn.softsign\n",
    "tf.nn.bias_add\n",
    "tf.sigmoid\n",
    "tf.tanh\n",
    "# 梯度截断\n",
    "gradients = tf.gradients(cost, tf.trainable_variables())\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, trainables))\n",
    "# 学习率的退火\n",
    "learning_rate = tf.train.exponential_decay(init_lr, global_step, decay_steps, decay_rate, staircase=True)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# dropout 防止过拟合\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
